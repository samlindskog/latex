\documentclass{article}
\usepackage{settings}

\geometry{
a4paper,
total={140mm,257mm},
left=35mm,
top=20mm,
}

\title{Complex Variables}
\author{Samuel Lindskog}

\begin{document}
\maketitle
\addtocontents{toc}{\protect\hypertarget{toc}{}}
\tableofcontents
\pagenumbering{gobble}
\clearpage
\pagenumbering{arabic}
\setcounter{page}{1}
\section{Complex numbers}
\subsection{Fundamental definitions and identities}
\begin{definition}[Complex number]
	A complex number is an expression of the form \(z=x+iy\), where \(x\) and \(y\) are real numbers.
\end{definition}
\begin{definition}
	Ever complex number \(z\neq 0\) has a multiplicative inverse given by
	\begin{equation*}
		\frac{1}{z}=\frac{x-iy}{x^2+y^2}.
	\end{equation*}
\end{definition}
\begin{definition}[Modulus]
	The modulus of a complex number \(z=x+iy\) is the length of the vector \((x,y)\), and is denoted \(\abs{z}\).
	\begin{equation*}
		\abs{z}=\sqrt{x^2+y^2}.
	\end{equation*}
\end{definition}
\begin{proposition}
	For \(z,w\in\mathbb{C}\), it follows from the triangle inequality that
	\begin{IEEEeqnarray*}{l}
		\abs{z+w}\leq\abs{z}+\abs{w}\\
		\abs{z-w}\geq\abs{z}-\abs{w}
	\end{IEEEeqnarray*}
\end{proposition}
\begin{definition}[Multiplication]
	\((x+iy)(u+iv)=xu-yv+i(xv+yu)\).
\end{definition}
\begin{definition}[Complex conjugate]
	The complex conjugate of a complex number \(z=x+iy\) is defined to be \(\overbar{z}=x-iy\).
\end{definition}
\begin{proposition}
	For \(z,w\in\mathbb{C}\), the following identities hold:
	\begin{IEEEeqnarray*}{l}
	\overbar{\overbar{z}}=z\\
		\overbar{z+w}=\overbar{z}+\overbar{w}\\
		\overbar{zw}=\overbar{z}\overbar{w}\\
		\overbar{z\overbar{w}}=\overbar{z}w\\
		\abs{z}=\abs{\overbar{z}}\\
		\abs{z}^2=z\overbar{z}\\
		\abs{zw}=\abs{z}\abs{w}\\
		\text{Re}\,z+\text{Re}\,w=\text{Re}\,z+w\\
		\text{Im}\,z+\text{Im}\,w=\text{Im}\,z+w
	\end{IEEEeqnarray*}
\end{proposition}
\begin{proposition}
	The real and imaginary parts of \(z\) can recovered from \(z\) by
	\begin{IEEEeqnarray*}{l}
		\text{Re}\,z=(z+\overbar{z})/2\\
		\text{Im}\,z=(z-\overbar{z})/2i
	\end{IEEEeqnarray*}
\end{proposition}
\begin{lemma}[Triangle inequality in \(\mathbb{R}^n\)]
	Suppose \(a,b\in\mathbb{R}^n\), with \(\abs{a}\) the distance from \(a\) to \(0\) under the euclidean metric. Then
	\begin{equation*}
		\abs{a+b}\leq\abs{a}+\abs{b}.
	\end{equation*}
\end{lemma}
\begin{IEEEproof}
	If dot product of two vectors is zero, they are LI. Prove basis exists such that each vector dotted with all vectors in basis is zero (use nullity potentially). if \(a,b\) vectors such that \(b\cdot a=0\), then \(a\cdot(a+b)=a\cdot a\). If \(\abs{a+b}<a\) then \(a\cdot(a+b)<a\cdot a\), so \(\abs{a+b}\geq\abs{a}\). \(\abs{a},\abs{b}\) are both geq than magnitude of their sides made of a scalar multiple of \(a+b\).
\end{IEEEproof}
\begin{proposition}
	\label{modsquared}
	Let \(a,b\in{\mathbb{C}}\). Then
	\begin{equation*}
		\abs{a+b}^2=\abs{a}^2+\abs{b}^2+a\overline{b}+b\overline{a}=\abs{a}^2+\abs{b}^2+2\text{Re}\,a\overline{b}.
	\end{equation*}
\end{proposition}
\begin{lemma}[Triangle inequality in \(\mathbb{C}\)]
	For \(x,y\in\mathbb{C}\), \(\abs{x+y}\leq\abs{x}+\abs{y}\).
\end{lemma}
\begin{IEEEproof}
	Suppose \(u,v\in\mathbb{R}\). Then
	\begin{IEEEeqnarray*}{l}
		\abs{u+iv}=\sqrt{u^2+v^2}\geq\sqrt{u^2}=\abs{u}\geq u.
	\end{IEEEeqnarray*}
	Therefore \(\text{Re}\,x+y\leq\abs{x+y}\) and
	\begin{IEEEeqnarray*}{l}
		2\text{Re}\,x\overbar{y}\leq2\abs{x\overbar{y}}=2\abs{xy}=2\abs{x}\abs{y}
	\end{IEEEeqnarray*}
	Because \((\abs{x}+\abs{y})^2=\abs{x}^2+\abs{y}^2+2\abs{x}\abs{y}\), it follows from proposition \ref{modsquared} that \((\abs{x}+\abs{y})^2\leq(\abs{x}+\abs{y})^2\), and therefore \(\abs{x+y}\leq\abs{x}+\abs{y}\).
\end{IEEEproof}
\begin{definition}[Cauchy's inequality]
	\begin{equation*}
		\bigg\lvert\sum_{i=1}^{n}a_ib_i\bigg\rvert^2\leq\sum_{i=1}^n\abs{a_i}^2+\sum_{i=1}^n\abs{b_i}^2
	\end{equation*}
\end{definition}
\begin{proposition}
	If \(z=x+iy\) then
	\begin{IEEEeqnarray*}{l}
		\abs{x}\leq\abs{z},\\
		\abs{y}\leq\abs{z}.
	\end{IEEEeqnarray*}
\end{proposition}
\begin{IEEEproof}
	From the definition of modulus,
	\begin{equation}
		\abs{z}^2=x^2+y^2.\label{modeqn}
	\end{equation}
	Thus wlog
	\begin{IEEEeqnarray*}{l}
		x=\pm\sqrt{\abs{z}^2-y^2},\\
		\abs{x}=\sqrt{\abs{z}^2-y^2}.
	\end{IEEEeqnarray*}
	Suppose to the contrary \(\abs{x}>\abs{z}\). Then \(\abs{x}^2=x^2>\abs{z}^2\), and because \(y^2\) is nonnegative, equation \ref{modeqn} is not true.
\end{IEEEproof}
\subsection{Polar representation}
\begin{definition}[Polar representation]
	The polar representation of a complex number \(z=x+iy\) is
	\begin{equation*}
		re^{i\theta}=r(\cos \theta+i\sin \theta).
	\end{equation*}
	Here \(r=\abs{z}\). The \emph{argument} of \(z\) is a multivalued function of \(\theta\), with
	\begin{equation*}
		\text{arg}\,z\in\{\theta+2\pi k\,|\,k\in\mathbb{Z}\}.
	\end{equation*}
	The principle value of \(\text{arg}\,z\) denoted \(\text{Arg}\,z\) is the unique member of \(\text{arg}\,z\) such that \(-\pi<\text{Arg}\,z\leq\pi\).
\end{definition}
\begin{remark}
	The value of certain functions depend on the argument of elements in it's domain. Thus in order to prevent the function from being multi-valued, a 'branch' of the domain must be chosen with only one argument. In order to maintain continuity, 'branch cuts' must be made to the functions domain, where the argument of elements on a branch coincide.
\end{remark}
\begin{definition}[de Moiver's formulae]
	The identies obtained by equating the imaginary and real parts of the expansions of \(e^{in\theta}\) and \((e^{in})^\theta\) are known as de Moivre's formulae, e.g.
	\begin{IEEEeqnarray*}{l}
		e^{2i\theta}=(e^{i\theta})^2\\
		\cos 2\theta+i\sin 2\theta=\cos^2 \theta+2i\cos \theta\sin \theta-\sin^2 \theta\\
		\cos 2\theta=\cos^2 \theta-\sin^2 \theta\\
		\sin 2\theta=2\cos\theta\sin \theta
	\end{IEEEeqnarray*}
\end{definition}
\begin{definition}[\(n\)th root]
	A number \(z\in\mathbb{C}\) is the \(n\)th root of \(w\in\mathbb{C}\) if \(z^n=w\). If \(w=\rho e^{i\varphi}\neq 0\), then the \(n\)th roots of \(w\) are
	\begin{equation*}
		\rho^{1/n}e^{i\varphi/n+2\pi k/n},\quad k=0,1,\ldots,n-1.
	\end{equation*}
	This is equivalent to multiplying \(\rho^{1/n}e^{i\varphi/n}\) by the \(n\)th roots of unity, i.e. all \(n\)th roots of \(1\).
\end{definition}
\subsection{Exp, log, and power functions}
\begin{definition}[Extended complex plane]
	The extended complex plane is the complex plane together with the point at infinity, denoted \(\mathbb{C}^*=\mathbb{C}\cup\{\infty\}\).
\end{definition}
\begin{proposition}
	If \(z\in\mathbb{C}\) with \(z=x+iy\) then
	\begin{equation*}
		e^z=e^xe^{iy}=e^{x}(\cos y+i\sin y).
	\end{equation*}
\end{proposition}
\section{Analytic Functions}
\begin{remark}
	Going forward, if a function's domain and codomain are not specified, the function is from \(\mathbb{C}\) to \(\mathbb{C}\).
\end{remark}
\subsection{Limits}
\begin{definition}[Limits]
	If the limit of \(f(z)\) as \(z\) approaches \(z_0\) is \(w_0\), this means that for all \(\epsilon\in\mathbb{R}^+\), there exists \(\delta\in\mathbb{R}^+\) such that
	\begin{equation*}
		0<\abs{z-z_0}<\delta\Rightarrow \abs{f(z)-w_0}<\epsilon.
	\end{equation*}
	This is written as \(\lim_{z\rightarrow z_0}f(z)=w_0\). If the domain or range of the function we are taking the limit of is \(\mathbb{R}^n\), the definition remains the same and uses the euclidean metric.
\end{definition}
\begin{lemma}
	The limit of a function is unique.
\end{lemma}
\begin{IEEEproof}
	Suppose \(f:\mathbb{C}\rightarrow\mathbb{C}\) a function and that \(\lim_{z\rightarrow z_0}f(z)=w_0\) and \(\lim_{z\rightarrow z_0}f(z)=w_1\) with \(w_0\neq w_1\). Because \(w_0\neq w_1\), \(\abs{w_0-w_1}=L>0\). If we take \(\epsilon=L/2\), there exists \(\delta>0\) such that \(\abs{z-z_0}<\delta\Rightarrow \abs{f(z)-w_0}<L/2\,\wedge\,\abs{f(z)-w_1}<L/2\). Because \((z-w_1)+(w_0-z)=w_0-w_1\), by the triangle inequality \(\abs{w_0-w_1}\leq\abs{f(z)-w_0}+\abs{f(z)-w_1}=L\), a contradiction.
\end{IEEEproof}
\begin{theorem}
	Suppose \(f(z)=u(x,y)+iv(x,y),\;z=x+iy\), and that \(z_0=x_0+iy_0\) and \(w_0=u_0+iv_0\). Then \(\lim_{z\rightarrow z_0}f(z)=w_0\) iff
	\begin{equation}
		\label{limreim}
		\lim_{(x,y)\rightarrow (x_0,y_0)}u(x,y)=u_0\text{ and }
		\lim_{(x,y)\rightarrow (x_0,y_0)}v(x,y)=v_0.
	\end{equation}
\end{theorem}
\begin{IEEEproof}
	Suppose \(\lim_{z\rightarrow z_0}f(z)=w_0\). Then for all \(\epsilon>0\) there exists \(\delta>0\) such that
	\begin{IEEEeqnarray*}{l}
		\abs{(x-x_0)+i(y-y_0)}=\sqrt{(x-x_0)^2+(y-y_0)^2}<\delta\Rightarrow\\
		\abs{u(x,y)-u_0+i(v(x,y)-v_0)}=\sqrt{(u(x,y)-u_0)^2+(v(x,y)-v_0)^2}<\epsilon.
	\end{IEEEeqnarray*}
	Because \(\sqrt{(u(x,y)-u_0)^2}\leq\sqrt{(u(x,y)-u_0)^2+(v(x,y)+v_0)^2}\),
	\begin{equation*}
		\sqrt{(x-x_0)^2+(y-y_0)^2}<\delta\Rightarrow\sqrt{(u(x,y)-u_0)^2}<\epsilon.
	\end{equation*}
	Therefore wlog \(\lim_{(x,y)\rightarrow(x_0,y_0)}u(x,y)=u_0\) and \(\lim_{(x,y)\rightarrow(x_0,y_0)}v(x,y)=v_0\).
	\medbreak
	\noindent Suppose \(\lim_{(x,y)\rightarrow(x_0,y_0)}u(x,y)=u_0\) and \(\lim_{(x,y)\rightarrow(x_0,y_0)}v(x,y)=v_0\). Then for all \(\epsilon>0\) there exists \(\delta_1,\delta_2>0\) such that
	\begin{IEEEeqnarray*}{l}
		\sqrt{(x-x_0)^2+(y-y_0)^2}<\delta_1\Rightarrow\sqrt{(u(x,y)-u_0)^2}<\epsilon/2,\\
		\sqrt{(x-x_0)^2+(y-y_0)^2}<\delta_1\Rightarrow\sqrt{(v(x,y)-v_0)^2}<\epsilon/2.
	\end{IEEEeqnarray*}
	If \(0<\delta<\delta_1,\delta_2\), it follows from the triangle inequality, the definition of \(f\), and the definition of modulus that
	\begin{equation*}
		\abs{z-z_0}<\delta\Rightarrow\abs{f(z)-w_0}<\epsilon.
	\end{equation*}
	Thus \(\lim_{z\rightarrow z_0}f(z)=w_0\).
\end{IEEEproof}
\begin{remark}
	Going forward, for \(x,y\in\mathbb{R}^n\), \(d(x,y)\) refers to the euclidean metric.
\end{remark}
\begin{theorem}
	Suppose that
	\begin{equation*}
		\lim_{z\rightarrow z_0}f(x)=w_0\text{ and }\lim_{z\rightarrow z_0}F(z)=W_0.
	\end{equation*}
	Then the following is true:
	\begin{IEEEeqnarray*}{l}
		\lim_{z\rightarrow z_0}[f(z)+F(z)]=w_0+W_0,\\
		\lim_{z\rightarrow z_0}[f(z)F(z)]=w_0W_0,\\
		\lim_{z\rightarrow z_0}\frac{f(z)}{F(z)}=\frac{w_0}{W_0},\quad W_0\neq 0.
	\end{IEEEeqnarray*}
\end{theorem}
\begin{IEEEproof}
prove
\end{IEEEproof}
\begin{theorem}
	If \(z_0\) and \(w_0\) are points in the \(z\) and \(w\) planes respectively, then the following properties hold:
	\begin{IEEEeqnarray*}{l}
		\lim_{z\rightarrow z_0}f(z)=\infty\Leftrightarrow\lim_{z\rightarrow z_0}\frac{1}{f(z)}=0\\
		\lim_{z\rightarrow\infty}f(z)=w_0\Leftrightarrow\lim_{z\rightarrow 0}f(\frac{1}{z})=w_0\\
		\lim_{z\rightarrow\infty}f(z)=\infty\Leftrightarrow\lim_{z\rightarrow 0}\frac{1}{f(1/z)}=0
	\end{IEEEeqnarray*}
\end{theorem}
\begin{IEEEproof}
	prove
\end{IEEEproof}
\subsection{Derivatives}
\begin{definition}[Continuity]
	A function \(f\) is continuous at a point \(z_0\) if all three of the following conditions are satisfied:
	\begin{enumerate}
		\item \(\lim_{z\rightarrow z_0}f(z)\) exists.
		\item \(f(z_0)\) exists.
		\item \(\lim_{z\rightarrow z_0}f(z)=f(z_0)\).
	\end{enumerate}
\end{definition}
\begin{theorem}
	The composition of continuous functions is continuous.
\end{theorem}
\begin{IEEEproof}
	prove
\end{IEEEproof}
\begin{theorem}
	If a function \(f(z)\) is continuous and nonzero at a point \(z_0\), then \(f(z)\neq 0\) throughout some neighborhood of that point
\end{theorem}
\begin{IEEEproof}
	prove
\end{IEEEproof}
\begin{theorem}
	If a function \(f\) is continuous throughout a region \(R\) that is both closed and bounded, there exists a nonnegative real number \(M\) and \(z'\in R\) such that
	\begin{equation*}
		\forall z\in R,\,\abs{f(z)}\leq M
	\end{equation*}
	and
	\begin{equation*}
		f(z')=M.
	\end{equation*}
\end{theorem}
\begin{IEEEproof}
	prove
\end{IEEEproof}
\begin{definition}[Derivative]
	Let \(f\) be a function whose domain of definition contains an \(\epsilon\)-neighborhood of \(z_0\). The derivative of \(f\) at \(z_0\) is the limit
	\begin{equation*}
		f'(z_0)=\lim_{z\rightarrow z_0}\frac{f(z)-f(z_0)}{z-z_0}.
	\end{equation*}
	The function \(f\) is said to be differentiable at \(z_0\) if \(f'(z_0)\) exists. If we set \(\Delta z=z-z_0\), we can write the definition as
	\begin{equation*}
		f'(z_0)=\lim_{\Delta z\rightarrow 0}\frac{f(z_0+\Delta z)-f(z_0)}{\Delta z}.
	\end{equation*}
	When using this form of the derivative, the subscript on \(z\) is often dropped and we introduce the number \(\Delta w=f(z+\Delta z)-f(z)\) so that the derivative becomes
	\begin{equation*}
		f'(z_0)=\diff{w}{z}=\lim_{\Delta z\rightarrow 0}\frac{\Delta w}{\Delta z}.
	\end{equation*}
\end{definition}
\begin{proposition}
	Because the derivative is a limit, if it exists it must be unique.
\end{proposition}
\begin{proposition}
	If a function \(f:\mathbb{C}\rightarrow \mathbb{C}\) is differentiable at a point \(z_0\in\mathbb{C}\), then \(f\) is continuous at \(z_0\).
\end{proposition}
\begin{proposition}[Differentiation formulas]
	Let \(c\in\mathbb{C}\) be a constant, \(z\in\mathbb{C}\) an independent variable, \(n\in\mathbb{Z}\), and \(f\) a function from \(\mathbb{C}\rightarrow\mathbb{C}\) which is differentiable at \(z\). These differentiation formulas can be derived from the definition of the derivative:
	\begin{IEEEeqnarray*}{l}
		\diff{}{z}c=0\\
		\diff{}{z}z=1\\
		\diff{}{z}[cf(z)]=cf'(z)\\
		\diff{}{z}z^n=nz^{n-1}\\
		\diff{}{z}[f(z)+g(z)]=f'(z)+g'(z)\\
		\diff{}{z}f(z)g(z)=f(z)g'(z)+f'(z)g(z)\\
		\diff{}{z}\frac{f(z)}{g(z)}=\frac{g(z)f'(z)-f(z)g'(z)}{[g(z)]^2}\\
	\end{IEEEeqnarray*}
\end{proposition}
\begin{theorem}[Chain rule]
	If \(f,g\) functions from \(\mathbb{C}\rightarrow\mathbb{C}\) differentiable at \(z\in\mathbb{C}\), then
	\begin{equation*}
		\diff{}{z}g\circ f(z)=g'\circ f(z)\cdot f'(z).
	\end{equation*}
\end{theorem}
\subsection{Cauchy-Riemann equations}
\begin{theorem}
	Suppose that \(f(z)=u(x,y)+iv(x,y)\) with \(z=x+iy\) and that \(f'(z)\) exists at a point \(z_0=x_0+iy_0\). Additionally, let \(u_x,v_x,u_y,v_y\) be the partial derivatives of the component functions of \(f\) with respect to \(x\) and \(y\) at \(x_0,y_0\). Then the first order partial derivatives of \(u\) and \(v\) exist at \(x_0,y_0\), and they must satisfy the Cauchy-Riemann equations
	\begin{equation*}
		u_x=v_y,\quad u_y=-v_x
	\end{equation*}
	at that point. In polar form, these equations are
	\begin{equation*}
		ru_r=v_\theta,\quad u_\theta=-rv_r.
	\end{equation*}
\end{theorem}
\begin{IEEEproof}
	The proof follows from the equality of the limit definition of the derivative approaching a point \(z_0\) from the \(x\)-axis, and approaching \(z_0\) from the \(y\)-axis. We utilize the fact that the limit of a sum of component functions of \(z\) is the sum of the limits of the individual functions. These individual limits then become the partial derivatives of \(f\) with respect to \(x\) and \(y\). Approaching \(z_0\) along the \(x\)-axis, the derivative of \(f=u(x,0)+v(x,0)\) at \(z_0\) is
	\begin{IEEEeqnarray*}{rCl}
		f'(z_0)&=&\lim_{\Delta x\rightarrow 0}\frac{u(x+\Delta x)-u(x)}{\Delta x}
		+i\frac{v(x+\Delta x)-v(x)}{\Delta x}\\
		&=&\lim_{\Delta x\rightarrow 0}\frac{u(x+\Delta x)-u(x)}{\Delta x}
		+\lim_{\Delta x\rightarrow 0}i\frac{v(x+\Delta x)-v(x)}{\Delta x}\\
	\end{IEEEeqnarray*}
	Approaching \(z_0\) along the \(y\)-axis, the derivative of \(f(u(0,y)+v(0,y))\) at \(z_0\) is
	\begin{IEEEeqnarray*}{rCl}
		f'(z_0)&=&\lim_{\Delta y\rightarrow 0}\frac{u(y+\Delta y)-u(y)}{i\Delta y}
		+i\frac{v(y+\Delta y)-v(y)}{i\Delta y}\\
		&=&\lim_{\Delta y\rightarrow 0}\frac{u(y+\Delta y)-u(y)}{i\Delta y}
		+\lim_{\Delta y\rightarrow 0}i\frac{v(y+\Delta y)-v(y)}{i\Delta y}\\
		&=&\lim_{\Delta y\rightarrow 0}-i\frac{u(y+\Delta y)-u(y)}{\Delta y}
		+\lim_{\Delta y\rightarrow 0}\frac{v(y+\Delta y)-v(y)}{\Delta y}\\
	\end{IEEEeqnarray*}
	Thus \(u_x=v_y\) and \(u_y=-v_x\).
\end{IEEEproof}
\begin{remark}
	\(1/i=-i\).
\end{remark}
\begin{corollary}
	\(f'(z_0)=u_x+iv_x=v_y-iu_y\).
\end{corollary}
\begin{theorem}
	Let the function \(f(z)=u(x,y)+iv(x,y)\) be defined throughout some \(\epsilon\)-neighborhood of a point \(z_0=x_0+iy_0\), and suppose that:
	\begin{enumerate}
		\item The first-order partial derivatives of the functions \(u\) and \(v\) with respect to \(x\) and \(y\) exist everywhere in the neighborhood.
		\item Those partial derivatives are continuous at \((x_0,y_0)\) and satisfy the Cauchy-Riemann equations at \((x_0, y_0)\).
	\end{enumerate}
	Then \(f'(z_0)\) exists.
\end{theorem}
\begin{IEEEproof}
	prove
\end{IEEEproof}
\begin{theorem}
	Let the function \(f(z)=u(r,\theta)+iv(r,\theta)\) be defined throughout some \(\epsilon\)-neighborhood of a nonzero point \(z_0=r_0\exp(i\theta_0)\), and suppose that
	\begin{enumerate}
		\item The first order partial derivatives of the function \(u\) and \(v\) with respect to \(r\) and \(\theta\) exists everywhere in the neighborhood;
		\item Those partial derivatives are continuous at \((r_0,\theta_0)\) and satisfy the polar form of the Cauchy Reimann equations.
	\end{enumerate}
	Then \(f'(z_0)\) exists, its value being \(f'(z_0)=\exp(i\theta_0)(u_r+iv_r)\).
\end{theorem}
\begin{IEEEproof}
	prove
\end{IEEEproof}
\subsection{Analytic and harmonic functions}
\begin{definition}[Analytic function]
	A function \(f\) of the complex variable \(z\) is analytic at a point \(z_0\) if it has a derivative at each point in some neighborhood \(z_0\). A function is analytic in an open set if it has a derivative everywhere in that set.
\end{definition}
\begin{definition}[Entire function]
	An entire function from \(\mathbb{C}\rightarrow \mathbb{C}\) is a function that is analytic at each point in its domain.
\end{definition}
\begin{remark}
	Every polynomial is an entire function.
\end{remark}
\begin{definition}[Singular point]
	If \(f:\mathbb{C}\rightarrow\mathbb{C}\) is a function not analytic at a point \(z_0\), but analytic at some point in every neighborhood of \(z_0\), then \(z_0\) is called a singular point.
\end{definition}
\begin{proposition}
	If two functions \(P\) and \(Q\) are analytic in a domain \(D\), their sum and product are analyitic. Their quotient is analytic in \(D\) provided the denominator is nonzero in \(D\).
\end{proposition}
\begin{proposition}
	The composition of two analytic functions is analytic.
\end{proposition}
\begin{theorem}
	If \(f'(z)=0\) everywhere in a domain \(D\), then \(f(z)\) must be constant throughout \(D\).
\end{theorem}
\begin{definition}[Harmonic]
	A real-valued function \(H\) of two real variables \(x\) and \(y\) is said to be harmonic in a given domain of the \(xy\) plane if, throughout that domain, it has continuous partial derivatives of the first and second order, and satisfies
	\begin{equation*}
		H_{xx}+H_{yy}=0.
	\end{equation*}
\end{definition}
\begin{theorem}
	If a function \(f(z)=u(x,y)+iv(x,y)\) is analytic in a domain \(D\), then it's component functions \(u\) and \(v\) are harmonic in \(D\).
\end{theorem}
\begin{definition}[Harmonic conjugate]
	If two given functions \(u\) and \(v\) are harmonic in a domain \(D\) and their first-order partial derivatives satisfy the Cauchy-Riemann equations (in the same order as \(u\) and \(v\) appear in the c.r. equations) throughout \(D\), then \(v\) is said to be the harmonic conjugate of \(u\).
\end{definition}
\begin{remark}
	It is possible that \(v\) is the harmonic conjugate of \(u\) with the reverse not being true.
\end{remark}
\begin{theorem}
	A function \(f(z)=u(x,y)+iv(x,y)\) is analytic in a domain \(D\) iff \(v\) is the harmonic conjugate of \(u\).
\end{theorem}
\subsection{read 27/28}
\section{Elementary functions}
\begin{definition}[Branch]
	A branch of a multiple-valued function \(f\) is any single-valued function \(F\) that is analytic in some domain at each point \(z\) of which the value \(F(z)\) is one of the values of \(f\).
\end{definition}
\begin{definition}[Complex exponents]
	When \(z\neq 0\) and the exponent \(c\) is any complex number, the function \(z^c\) is defined by means of the equation
	\begin{equation*}
		z^c=e^{c\log z}.
	\end{equation*}
\end{definition}
\begin{definition}[log and Log]
	Let \(z=re^{i\theta}\). \(\log z\) is a multiple-valued function defined by
	\begin{equation*}
		\log z=\ln \abs{z}+i\arg z.
	\end{equation*}
	\(\text{Log}\,z\) is the single valued function
	\begin{equation*}
		\text{Log}\,z=\ln r+i\Theta,
	\end{equation*}
	Where \(\Theta=\text{Arg}\,z\). \(\text{Log}\,z\) is not analytic because it is not continuous when \(z\in\mathbb{R}^{-}\).
\end{definition}
\begin{proposition}[Branches and derivatives of logarithms]
	If \(\theta=\Theta+2n\pi\) for \(n\in\mathbb{Z}\), and \(z=re^{i\theta}\), then \(\log z\) can be defined
	\begin{equation*}
		\log z=\ln r+i\theta.
	\end{equation*}
	If we let \(\alpha\) denoted any real number, and \(\alpha<\theta<\alpha+2\pi\) with \(r>0\), \(\log z\) becomes a single-valued continuous function. \(\log z\) restricted to this domain is a branch of \(\log z\). If \(\alpha=-\pi\), this branch is called the principal branch, and is equal to \(\text{Log}\, z\) with the additional restriction \(\Theta<\pi\).
\end{proposition}
\begin{definition}[Sin and Cos]
	\begin{IEEEeqnarray*}{l}
		\sin x=\frac{e^{ix}-e^{-ix}}{2i}\\
		\cos x=\frac{e^{ix}+e^{-ix}}{2}
	\end{IEEEeqnarray*}
\end{definition}
\begin{definition}[Sinh and Cosh]
	\begin{IEEEeqnarray*}{rCl}
		\sinh x=\frac{e^x-e^{-x}}{2}\\
		\cosh x=\frac{e^x+e^{-x}}{2}
	\end{IEEEeqnarray*}
\end{definition}
\begin{proposition}
	\begin{IEEEeqnarray*}{c}
		\abs{\sin z}^2=\sin^2 x+\sinh^2 y\\
		\abs{\cos z}^2=\cos^2 x+\sinh^2 y
	\end{IEEEeqnarray*}
\end{proposition}
\section{Integrals}
\subsection{Derivatives and contours}
\begin{definition}[Derivative]
	Let \(w(t)=u(t)+iv(t)\) be a complex-valued function of a real variable, where the functions \(u(t)\) and \(v(t)\) are real valued functions of a real variable. Then the derivative of \(w(t)\) with respect to \(t\) is
	\begin{equation*}
		\diff{}{t}w(t)=\diff{}{t}u(t)+i\diff{}{t}v(t).
	\end{equation*}
\end{definition}
\begin{definition}[Definite integral for function of a real variable]
	When \(w(t)\) is a complex-valued function of a real variable \(t\), written
	\begin{equation*}
		w(t)=u(t)+iv(t),
	\end{equation*}
	where \(u\) and \(v\) are real-valued, the definite integral of \(w(t)\) over an interval \(a\leq t\leq b\) is
	\begin{equation*}
		\int_a^b w(t)dt=\int_a^b u(t)dt+i\int_a^b v(t)dt,
	\end{equation*}
	Provided the integrals on the left exist.
\end{definition}
\begin{remark}
	This is analagous to derivatives of vector functions in calculus, where the definite integral of a function \(f:\mathbb{R}\rightarrow\mathbb{R}^n\) is a vector in \(\mathbb{R}^n\).
\end{remark}
\begin{definition}[Arc]
	A set of points \(z=(x,y)\) in the complex plane is said to be an arc if
	\begin{equation*}
		x=x(t),\quad y=y(t),\quad a\leq t\leq b
	\end{equation*}
	where \(x\) and \(y\) are continuous functions of the real parameter \(t\). An arc \(C\) is called a simple arc, or a Jordan arc, if it does not cross itself. When the arc is simple except for the fact that \(z(b)=z(a)\), we say that \(C\) is a simple closed curve. Such a curve is positively oriented when it is in the counterclockwise direction.
\end{definition}
\begin{definition}[Smooth arc]
	A smooth arc \(z=z(t)\) defined on \(a\leq t\leq b\) has continuous first derivatives on its domain \(a\leq t\leq b\) which are nonzero on \(a<t<b\).
\end{definition}
\begin{definition}[Contour]
	A contour, or piecewise smooth arc, is an arc consisting of a finite number of smooth arcs joined end to end. When only the initial and final values of a contour \(C\) are the same, we say \(C\) is a simple closed contour.
\end{definition}
\begin{theorem}[Jordan curve theorem]
	The points on any simple closed contour \(C\) are boundary points of two distinct domains. One of these domains is the interior of \(C\), and is bounded. The other is the exterior of \(C\), and is unbounded.
\end{theorem}
\subsection{Contour integrals}
Suppose a contour \(C\) is represented by the function \(z:\mathbb{R}\rightarrow\mathbb{C}\) on the interval \((a\leq t\leq b)\). If \(f:\mathbb{C}\rightarrow\mathbb{C}\) a function, and \(f[z(t)]\) is piecewise continuous on the interval \(a\leq t\leq b\), the function \(f\) is piecewise continuous on \(C\). We then define the contour integral of \(f\) along \(C\) as
\begin{equation*}
	\int_{C}f(z)dz=\int_a^b f[z(t)]z'(t)dt.
\end{equation*}
\begin{proposition}
	It follows from the properties of complex-valued functions of a real variable that
	\begin{IEEEeqnarray*}{l}
		\int_C z_0f(z)dz=z_0\int_Cf(z)dz,\\
		\int_C\big[f(z)+g(z)\big]dz=\int_Cf(z)dz+\int_Cg(z)dz.
	\end{IEEEeqnarray*}
\end{proposition}
\begin{remark}
	prove statements of section 40, 41, 42
\end{remark}
\begin{proposition}
\end{proposition}
\subsection{Contour integral other shit}
\begin{lemma}
	If \(w:\mathbb{R}\rightarrow\mathbb{C}\) is piecewise continuous on an interval \(a\leq t\leq b\), then
	\begin{equation*}
		\abs{\int_a^bw(t)dt}\leq\int_a^b\abs{w(t)}dt.
	\end{equation*}
\end{lemma}
\begin{theorem}
	Let \(C\) denote a contour of length \(L\), and suppose that a function \(f:\mathbb{C}\rightarrow\mathbb{C}\) is continuous on \(C\). If \(M\) is a nonnegative constant such that
	\begin{equation*}
		\abs{f(z)}\leq M,
	\end{equation*}
	for all points \(z\) on \(C\) at which \(f(z)\) is defined, then
	\begin{equation*}
		\abs{\int_Cf(z)dz}\leq ML.
	\end{equation*}
\end{theorem}
\begin{theorem}
	Suppose that a function \(f:\mathbb{C}\rightarrow\mathbb{C}\) is continuous on domain \(D\). The following statements are logically equivalent:
	\begin{enumerate}
		\item \(f\) has an antiderivative \(F\) throughout \(D\).
		\item The integrals of \(f\) along contours lying entirely in \(D\) and extending from any fixed point \(z_1\) to any fixed point \(z_2\) all have the same value.
		\item The integrals around closed contours lying entirely in \(D\) are equal to zero.
	\end{enumerate}
\end{theorem}
\begin{theorem}[Cauchy-Goursat theorem]
\item If a function \(f:\mathbb{C}\rightarrow\mathbb{C}\) is analytic at all points interior to and on a simple closed contour \(C\), then
	\begin{equation*}
		\int_C f(z)dz=0.
	\end{equation*}
\end{theorem}
\begin{theorem}
	If a function \(f:\mathbb{C}\rightarrow\mathbb{C}\) is analytic througout a simply connected domain \(D\), then
	\begin{equation*}
		\int_C f(z)dz=0
	\end{equation*}
	for every closed contour \(C\) lying in \(D\).
\end{theorem}
\begin{remark}
	Notice the lack of specificity that this contour is simple.
\end{remark}
\begin{theorem}
	Suppose that
	\begin{enumerate}
		\item \(C\) is a simple closed contour, described in the counterclockwise direction.
		\item \(C_k,\,k=1,\ldots,n\) are simple closed contours interior to \(C\), all described in the clockwise direction, that are disjoint and whose interiors have no points in common.
	\end{enumerate}
	If a function \(f\) is analytic on all of these contours and throughout the multiply connected domain consisting of the points inside \(C\) and exterior to each \(C_k\), then
	\begin{equation*}
		\int_Cf(z)dz+\sum_{k=1}^n\int_{C_k}f(z)dz=0.
	\end{equation*}
\end{theorem}
\begin{corollary}[Principle of deformation of paths]
	Let \(C_1\) and \(C_2\) denote positively oriented simple closed contours, where \(C_1\) is interior to \(C_2\). If a function \(f\) is analytic in the closed region consisting of these contours and all points between them, then
	\begin{equation*}
		\int_{C_1}f(z)dz=\int_{C_2}f(z)dz.
	\end{equation*}
\end{corollary}
\begin{theorem}[Cauchy integral formula]
	Let \(f:\mathbb{C}\rightarrow\mathbb{C}\) be analytic everywhere inside and on a simple closed contour \(C\), taken in the positive sense. If \(z_0\) is any point interior to \(C\), then
	\begin{equation*}
		f(z_0)=\frac{1}{2\pi i}\int_C\frac{f(z)dz}{z-z_0}.
	\end{equation*}
	This formula can be extended to find the derivative of functions analytic on and inside the interior of a simple closed contour \(C\) with repect to a point \(z\) on the interior of this curve:
	\begin{equation*}
		f^{(n)}(z)=\frac{n!}{2\pi i}\int_C\frac{f(s)ds}{(s-z)^{n+1}},\;n\in\mathbb{Z}^+.
	\end{equation*}
\end{theorem}
\begin{theorem}
	If a function \(f\) is analytic at a given point, then it's derivatives of all orders are analytic there too.
\end{theorem}
\begin{remark}
	read 51-53
\end{remark}
\begin{lemma}
	Suppose that \(\abs{f(z)}\leq\abs{f_{z_0}}\) at each point \(z\) in some neighborhood \(\abs{z-z_0}<\epsilon\) in which \(f\) is analytic. Then \(f(z)\) has constant value \(f(z_0)\) throughout that neighborhood.
\end{lemma}
\begin{theorem}
	If a function \(f\) is analytic and not constant in a given domain \(D\), then \(\abs{f(z)}\) has non maximum value in \(D\). That is, there is no point \(z_0\) in the domain such that \(\abs{f(z)}\leq\abs{f(z_0)}\) for all points \(z\) in it.
\end{theorem}
\begin{corollary}
	Suppose that a function \(f\) is continuous on a closed bounded region \(R\), and that it is analytic and not constant in the interior of \(R\). then the maximum value of \(\abs{f(z)}\) in \(R\), which is always reached, occurs somewhere on the boundary of \(R\) and never in the interior.
\end{corollary}
\begin{lemma}
	Suppose that \(\abs{f(z)}\leq\abs{f(z_0)}\) at each point \(z\) in some neighborhood \(\abs{z-z_0}<\epsilon\) in which \(f\) is analytic. Then \(f(z)\) has constant value \(f(z_0)\) throughout that neighborhood.
\end{lemma}
\begin{theorem}
	If a function \(f\) is analytic and not constant in a given domain \(D\), then \(\abs{f(z)}\) has no maximum value in \(D\). That is, there is no point \(z_0\) in the domain such that \(\abs{f(z)}\leq\abs{f(z_0)}\) for all points \(z\in D\).
\end{theorem}
\section{Sequences}
\subsection{Convergence}
\begin{definition}[Convergence of sequences]
A sequence \((z_n)_{n=1}^{\infty}\) of complex numbers has a limit \(z\) if
\begin{equation*}
	\forall\epsilon>0,\,\exists N\in\mathbb{N},\,\forall n\in\mathbb{N},\,\big(n>N\Rightarrow\abs{z_n-z}<\epsilon\big)
\end{equation*}
When a sequence has a limit, it is said to converge.
\end{definition}
\begin{theorem}
	Suppose that \(z_n=x_n+iy_n\). Then \(\lim_{n\rightarrow\infty}z_n=z\) iff
	\begin{equation*}
		\lim_{n\rightarrow\infty}x_n=x,\quad\lim_{n\rightarrow\infty}y_n=y.
	\end{equation*}
\end{theorem}
\begin{definition}[Convergence of series]
	An infinite series
	\begin{equation*}
		\sum_{n=1}^{\infty}z_n=z_1+z_2+\ldots
	\end{equation*}
	of complex numebrs converges to the sum \(S\) if the sequence
	\begin{equation*}
		S_N=\sum_{n=1}^{N}z_n
	\end{equation*}
	of partial sums converges to \(S\). We then write
	\begin{equation*}
		\sum_{n=1}^{\infty}z_n=S.
	\end{equation*}
\end{definition}
\begin{theorem}
	Suppose that \(z_n=x_n+iy_n\) and \(S=X+iY\). Then
	\begin{equation*}
		\sum_{n=1}^{\infty}z_n=S\Leftrightarrow\sum_{n=1}^{\infty}x_n=X\text{ and }
		\sum_{n=1}^{\infty}y_n=Y.
	\end{equation*}
\end{theorem}
\begin{corollary}
	If a series of complex numbers converges, the \(n\)th term converges to zero as \(n\) tends to infinity.
\end{corollary}
\subsection{Taylor series}
\begin{theorem}
	Suppoe that a function \(f\) is analytic throughout a disk \(\abs{z-z_0}<R_0\), centered at \(z_0\) and with radius \(R_0\). Then \(f(z)\) has the power series representation
	\begin{equation*}
		f(z)=\sum_{n=0}^{\infty}a_n(z-z_0)^n\quad(\abs{z-z_0}<R_0),
	\end{equation*}
	where
	\begin{equation*}
		a_n=\frac{f^{(n)}(z_0)}{n!}\quad n\in\mathbb{Z}^+.
	\end{equation*}
\end{theorem}
\begin{definition}[Maclaurin series]
	A Maclaurin series is a Taylor series centered at \(z_0=0\).
\end{definition}
\begin{theorem}
	Suppose that a function \(f\) is analytic throughout an annular domain \(R_1<\abs{z-z_0}<R_2\), centered at \(z_0\), and let \(C\) denote any positively oriented simple closed contour around \(z_0\) and lying in that domain. Then, at each point in the domain, \(f(z)\) has the series representation
	\begin{IEEEeqnarray*}{c}
		f(z)=\sum_{n=0}^{\infty}a_n(z-z_0)^n+\sum_{n=1}^\infty\frac{b_n}{(z-z_0)^n}\quad(R_1<\abs{z-z_0}<R_2).
	\end{IEEEeqnarray*}
	where
	\begin{equation*}
		a_n=\frac{1}{2\pi i}\int_C\frac{f(z)dz}{(z-z_0)^{n+1}}\quad n=0,1,\ldots,
	\end{equation*}
	and
	\begin{equation*}
		b_n=\frac{1}{2\pi i}\int_C\frac{f(z)dz}{(z-z_0)^{-n+1}}\quad n=1,2,\ldots.
	\end{equation*}
\end{theorem}
\subsection{Absolute and uniform convergence}
\begin{definition}[Absolute convergence]
	A series of complex numbers converges absolutely if the series of absolute values of those numbers converges.
\end{definition}
\begin{theorem}
	If a power series
	\begin{equation*}
		\sum_{n=0}^\infty a_n(z-z_0)^n
	\end{equation*}
	converges when \(z=z_1\) with \(z_1\neq z_0\), then it is absolutely convergent at each point \(z\) in the open disk \(\abs{z-z_0}<R_1\) where \(R_1=\abs{z_1-z_0}\).
\end{theorem}
\begin{remark}
	The greatest circle centered at \(z_0\) such that the above series converges is called the circle of convergence.
\end{remark}
\begin{remark}
	when proving this, go over uniform convergence.
\end{remark}
\begin{theorem}
	A power series
	\begin{equation*}
		\sum_{n=0}^{\infty}a_n(z-z_0)^n
	\end{equation*}
	represents a continuous function \(S(z)\) at each point inside its circle of convergence \(\abs{z-z_0}=R\).
\end{theorem}
\subsection{Integration and differentiation of power series}
\begin{theorem}
	Let \(C\) denote any contour interior to the circle of convergence of the power series
	\begin{equation}
		S(z)=\sum_{n=0}^{\infty}a_n(z-z_0)^n,\label{sz}
	\end{equation}
	and let \(g(z)\) be any function that is continuous on \(C\). The series formed by multiplying each term of the power series by \(g(z)\) can be integrated term by term over \(C\), i.e.
	\begin{equation*}
		\int_Cg(z)S(z)dz=\sum_{n=1}^{\infty}a_n\int_Cg(z)(z-z_0)^ndz.
	\end{equation*}
\end{theorem}
\begin{corollary}
	The sum \(S(z)\) of power series in equation \ref{sz} is analytic at each point \(z\) interior to the circle of convergence of that series.
\end{corollary}
\begin{theorem}
	The power series in equation \ref{sz} can be differentiated term by term, i.e. at each point \(z\) interior to the circle of convergence of that series,
	\begin{equation*}
		S'(z)=\sum_{n=1}^{\infty}na_n(z-z_0)^{n-1}.
	\end{equation*}
\end{theorem}
\begin{theorem}
	If a series
	\begin{equation*}
		\sum_{n=0}^{\infty}a_n(z-z_0)^n
	\end{equation*}
	converges to \(f(z)\) at all points interior to some circle \(\abs{z-z_0}=R\), then it is the Taylor series expansion for \(f\) in powers of \(z-z_0\).
\end{theorem}
\begin{theorem}
	If a series
	\begin{equation*}
		\sum_{n=-\infty}^{\infty}c_n(z-z_0)^n=\sum_{n=0}^\infty a_n(z-z_0)^n+\sum_{n=1}^{\infty}\frac{b_n}{(z-z_0)^n}
	\end{equation*}
	Converges to \(f(z)\) at all points in some annular domain about \(z_0\), then it is the Laurent series expansion for \(f\) in powers of \(z-z_0\) for that domain.
\end{theorem}
\subsection{Multiplication and division of power series}
\begin{remark}
	read 67
\end{remark}
\section{Residues and Poles}
\subsection{Residues}
\begin{definition}
	The residue of \(f\) at a point \(z_0\) is the coefficient \(b_1\) for the Laurent series valid in a deleted neighborhood of \(z_0\), i.e.
	\begin{equation*}
		2\pi i\text{Res}_{z=z_0}\,f(z)=\int_C f(z)dz.
	\end{equation*}
	The residue at infinity is \(b_1\) term for the Laurent series valid as \(\abs{z}\rightarrow\infty\).
\end{definition}
\begin{remark}
	The Cauchy-Goursat theorem implies that if \(f\) is analytic inside and on a simple closed curve \(C\), the residue of \(f\) at any point interior to \(C\) must be zero. Alternatively, if \(f\) is analytic, all coefficients on the principle part of the laurent series must be zero due to the integrand for these coefficients becoming analytic. It should be noted that a functions analyticity on a simply connected domain is a sufficient but not necessary condition for the integrals of simple closed curves being zero on that domain.
\end{remark}
\subsection{Classifying singular points}
\begin{definition}[Singular point]
	A point \(z_0\) is called a singular point of \(f\) if \(f\) fails to be analytic at \(z_0\) but is analytic at some point in every neighborhood of \(z_0\).
\end{definition}
\begin{definition}[Isolated point]
	A singular point is said to be isolated if there is a deleted neighborhood of \(z_0\) throughout which \(f\) is analytic.
\end{definition}
\begin{theorem}
	Let \(C\) be a simple closed contour, described in the positive sense. If a function \(f\) is analytic inside and on \(C\) except for a finite number of singular points \(z_k\) (\(k\in\mathbb{Z}^+\)) inside \(C\), then
	\begin{equation*}
		\int_C f(z)dz=2\pi i\sum_{k=1}^{n}\text{Res}_{z=z_k}f(z).
	\end{equation*}
\end{theorem}
\begin{definition}[Types of singular points]
	If the principle part of the Laurent series at an isolated singular point has a minimum degree \(-m\), then pole is called a pole of order \(m\). A pole of order \(m=1\) is referred to as a simple pole.\medbreak
	If the principle part of the Laurent series at an isolated singular point has all zero coefficients, the singularity at \(z_0\) is said to be removable.\medbreak
	If an infinite number of the coefficients in the principle part are nonzero, \(z_0\) is said to be an essential singular point of \(f\).
\end{definition}
\begin{theorem}
	An isolated singular point \(z_0\) of a function \(f\) is a pole of order \(m\) iff \(f(z)\) can be written in the form
	\begin{equation*}
		f(z)=\frac{\phi(z)}{(z-z_0)^m},
	\end{equation*}
	where \(\phi(z)\) is analytic and nonzero at \(z_0\). Moreover,
	\begin{equation*}
		\text{Res}\,_{z=z_0}f(z)=\phi(z_0),\;\text{if }m=1
	\end{equation*}
	and
	\begin{equation*}
		\text{Res}\,_{z=z_0}f(z)=\frac{\phi^{(m-1)}(z_0)}{(m-1)!}\;\text{if }m\geq 2.
	\end{equation*}
\end{theorem}
\begin{definition}[Zeros of analytic functions]
	Suppose a function \(f\) is analytic at a point \(z_0\). If \(f(z_0)=0\), and if there is a positive integer \(m\) with \(f^{(m)}(z_0)\neq 0\) such that \(n>m\) implies \(f^{(n)}(z_0)=0\), then \(f\) is said to have a zero of order \(m\) at \(z_0\).
\end{definition}
\begin{theorem}
	Let a function \(f\) be analytic at a point \(z_0\). It has a zero of order \(m\) at \(z_0\) iff there is a function \(g\), which is analytic and nonzero at \(z_0\), such that
	\begin{equation*}
		f(z)=(z-z_0)^mg(z).
	\end{equation*}
\end{theorem}
\begin{theorem}
	Suppose that
	\begin{enumerate}
		\item Two functions \(p\) and \(q\) are analytic at a point \(z_0\);
		\item \(p(z_0)\neq 0\) and \(q\) has a zero of order \(m\) at \(z_0\).
	\end{enumerate}
	Then the quotient \(p(z)/q(z)\) has a pole of order \(m\) at \(z_0\).
\end{theorem}
\begin{theorem}
	Let two functions \(p\) and \(q\) be analytic at a point \(z_0\). If
	\begin{equation*}
		p(z_0)\neq 0,\;q(z_0)=0,\;q'(z_0)\neq 0,
	\end{equation*}
	then \(z_0\) is a simple pole of the quotient \(p(z)/q(z)\) and
	\begin{equation*}
		\text{Res}_{z=z_0}\,\frac{p(z)}{q(z)}=\frac{p(z_0)}{q'(z_0)}.
	\end{equation*}
\end{theorem}
\section{Mappings by elementary functions}
\begin{definition}[Bilinear transformation]
	The transformation
	\begin{equation*}
		w=\frac{az+b}{cz+d},\;(ad-bc\neq 0),
	\end{equation*}
	where \(a,b,c,d\in\mathbb{C}\), is called a linear fractional transformation. This is a bijective mapping from the extended \(z\)-plane to the extended \(w\)-plane. Solving this equation for \(z\) yields
	\begin{equation*}
		z=\frac{-dw+b}{cw-a},\;(ad-bc\neq 0).
	\end{equation*}
	The equation
	\begin{equation*}
		\frac{(w-w_1)(w_2-w_3)}{(w-w_3)(w_2-w_1)}=\frac{(z-z_1)(z_2-z_3)}{(z-z_3)(z_2-z_1)}
	\end{equation*}
	Defines a linear fractional transformation that maps \(z_1,z_2,z_3\) in the finite \(z\) plane onto distinct points \(w_1,w_2,w_3\) respectively in the finite \(w\) plane.
\end{definition}
\end{document}
